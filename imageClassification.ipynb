{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JDjpPPsJ0sjx",
        "Tq-sdA1S07cS",
        "T2Jlymwl069W"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mullen44/CapstoneProject_SkinLesionClassification/blob/main/imageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDjpPPsJ0sjx"
      },
      "source": [
        "# GPU Allocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clh7HGPI0O-1"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "# retrieve GPU type\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]\n",
        "# check allocation of GPU resources\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ei2XY5K0ylk"
      },
      "source": [
        "# Import Modules and Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6l30q0l2PTn"
      },
      "source": [
        "# Mounting Google Drive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocTJfUAC2qj4"
      },
      "source": [
        "# Import Necessary Libraries and personal scripts (Utils.py & Models.py)\n",
        "# Tensorflow, matplotlib, numpy etc\n",
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/Scripts/\n",
        "from Utils import *\n",
        "from models import *\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import imsave, imread\n",
        "import matplotlib.pyplot as plt\n",
        "import math as mth\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50, VGG16, VGG19, InceptionV3, InceptionResNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "import random\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVLucJyp0yFT"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "print(len(os.listdir('Data/deep/bkl/')))\n",
        "print(len(os.listdir('Data/deep/mel/')))\n",
        "print(len(os.listdir('Data/deep/nv/')))\n",
        "print(len(os.listdir('Data/deep/test/')))\n",
        "print(len(os.listdir('Data/deep/train/')))\n",
        "print(len(os.listdir('Data/log/Features/bkl/')))\n",
        "print(len(os.listdir('Data/log/Features/mel/')))\n",
        "print(len(os.listdir('Data/log/Features/nv')))\n",
        "\n",
        "train = np.load('Data/log/Features/train.npy')\n",
        "print(len(train))\n",
        "test = np.load('Data/log/Features/test.npy')\n",
        "print(len(test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Jlymwl069W"
      },
      "source": [
        "# Divide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx7DIqRVmAic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc9a4d2-5758-41e6-9407-e7a282d96648"
      },
      "source": [
        "# Change to Directory where data is stored\n",
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/\n",
        "\n",
        "# Read in the metadata information about the images\n",
        "meta = pd.read_csv('HAM10000_metadata.csv')\n",
        "# Create a dictionary to store images\n",
        "data = dict()\n",
        "\n",
        "# Loop through and fill in data variable with key=filename and value=image array\n",
        "for filename in os.listdir('HAM10000_images_part_1/') :\n",
        "  data[filename[:-4]] = np.array(imread('HAM10000_images_part_1/' + filename))\n",
        "\n",
        "for filename in os.listdir('HAM10000_images_part_2/') :\n",
        "  data[filename[:-4]] = np.array(imread('HAM10000_images_part_2/' + filename))\n",
        "\n",
        "# Specify path to save labelled images in\n",
        "melPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/mel/'\n",
        "bklPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/bkl/'\n",
        "akiecPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/akiec/'\n",
        "bccPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/bcc/'\n",
        "dfPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/df/'\n",
        "nvPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/nv/'\n",
        "vascPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/vasc/'\n",
        "\n",
        "for i in range(len(meta)) :\n",
        "  if meta.image_id[i] in data :\n",
        "    if meta.dx[i] == 'akiec' :\n",
        "      # Save in akiec Folder\n",
        "      name = akiecPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'mel' :\n",
        "      # Save in Mel Folder\n",
        "      name = melPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'bkl' :\n",
        "      # Save in BKL folder\n",
        "      name = bklPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'bcc' :\n",
        "      # Save in bcc Folder\n",
        "      name = bccPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'df' :\n",
        "      # Save in df Folder\n",
        "      name = dfPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'nv' :\n",
        "      # Save in nv Folder\n",
        "      name = nvPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])\n",
        "    if meta.dx[i] == 'vasc' :\n",
        "      # Save in vasc Folder\n",
        "      name = vascPath + meta.image_id[i] + '.jpg'\n",
        "      imsave(name, data[meta.image_id[i]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUZ_mpP824zF"
      },
      "source": [
        "# Load Preprocessed images\n",
        "melPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/mel/'\n",
        "bklPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/bkl/'\n",
        "nvPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/nv/'\n",
        "\n",
        "mel = dict()\n",
        "bkl = dict()\n",
        "nv = dict()\n",
        "\n",
        "i = 0\n",
        "for filename in os.listdir(melPath) :\n",
        "  mel[filename] = imread(melPath + filename)\n",
        "  print(i)\n",
        "  i = i+1\n",
        "\n",
        "for filename in os.listdir(bklPath) :\n",
        "  bkl[filename] = imread(bklPath + filename)\n",
        "  print(i)\n",
        "  i = i+1\n",
        "\n",
        "for filename in os.listdir(nvPath) :\n",
        "  nv[filename] = imread(nvPath + filename)\n",
        "  print(i)\n",
        "  i = i+1\n",
        "\n",
        "\n",
        "# Divide data (Images from 3 classes) into training(60%), Validation(20%), Testing(20%)\n",
        "train_percent = 0.8\n",
        "val_percent = 0.25 * train_percent \n",
        "test_percent = 0.2\n",
        "\n",
        "# Create index variables\n",
        "indexMel = createIndex(mel, train_percent, val_percent, test_percent)\n",
        "indexBkl = createIndex(bkl, train_percent, val_percent, test_percent)\n",
        "indexNv = createIndex(nv, train_percent, val_percent, test_percent)\n",
        "\n",
        "# Make sure its randomized to avoid any bias\n",
        "# Save divided data into directories\n",
        "dataPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/'\n",
        "saveImagesToFolders(mel, indexMel, 'mel/', dataPath)\n",
        "saveImagesToFolders(bkl, indexBkl, 'bkl/', dataPath)\n",
        "saveImagesToFolders(nv, indexMel, 'nv/', dataPath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SotMBPiHygvY"
      },
      "source": [
        "print('akiec', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/akiec')))\n",
        "print('mel', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/mel')))\n",
        "print('bcc', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/bcc')))\n",
        "print('bkl', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/bkl')))\n",
        "print('df', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/df')))\n",
        "print('nv', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/nv')))\n",
        "print('vasc', len(os.listdir('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/DummyData/vasc')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Y-sH7BW0xj"
      },
      "source": [
        "# New Data Division"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd30RnJbNOfT"
      },
      "source": [
        "melPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/mel/'\n",
        "bklPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/bkl/'\n",
        "nvPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/DummyData/nv/'\n",
        "\n",
        "mel = dict()\n",
        "bkl = dict()\n",
        "nv = dict()\n",
        "\n",
        "for filename in os.listdir(melPath) :\n",
        "  mel[filename] = imread(melPath + filename)\n",
        "\n",
        "for filename in os.listdir(bklPath) :\n",
        "  bkl[filename] = imread(bklPath + filename)\n",
        "\n",
        "for filename in os.listdir(nvPath) :\n",
        "  nv[filename] = imread(nvPath + filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LOdmApdZ2wL"
      },
      "source": [
        "len(bkl)//2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHWyaFd4PM3H"
      },
      "source": [
        "# DeepLearning folders\n",
        "melDeepPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/mel/'\n",
        "bklDeepPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/bkl/'\n",
        "nvDeepPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/nv/'\n",
        "# Logistic Regression Folders\n",
        "melLogPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/log/mel/'\n",
        "bklLogPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/log/bkl/'\n",
        "nvLogPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/log/nv/'\n",
        "# State Dictionarys\n",
        "bklDeep = dict()\n",
        "melDeep = dict()\n",
        "nvDeep = dict()\n",
        "\n",
        "i = 0\n",
        "for key, value in bkl.items() :\n",
        "  if i <= len(bkl)//2 :\n",
        "    imsave(bklDeepPath + key, value)\n",
        "    bklDeep[key] = value\n",
        "  else :\n",
        "    imsave(bklLogPath + key, value)\n",
        "  i += 1\n",
        "print('Done BKL')\n",
        "\n",
        "i = 0\n",
        "for key, value in mel.items() :\n",
        "  if i <= len(mel)//2 :\n",
        "    imsave(melDeepPath + key, value)\n",
        "    melDeep[key] = value\n",
        "  else :\n",
        "    imsave(melLogPath + key, value)\n",
        "  i += 1\n",
        "print('Done MEL')\n",
        "\n",
        "i = 0\n",
        "for key, value in  nv.items() :\n",
        "  if i <= len(nv)//2 :\n",
        "    imsave(nvDeepPath + key, value)\n",
        "    nvDeep[key] = value\n",
        "  else :\n",
        "    imsave(nvLogPath + key, value)\n",
        "  i += 1\n",
        "print('Done NV')\n",
        "\n",
        "# Save into train val and test\n",
        "# Divide data (Images from 3 classes) into training(60%), Validation(20%), Testing(20%)\n",
        "train_percent = 0.8\n",
        "val_percent = 0.25 * train_percent \n",
        "test_percent = 0.2\n",
        "\n",
        "# Create index variables\n",
        "indexMel = createIndex(melDeep, train_percent, val_percent, test_percent)\n",
        "indexBkl = createIndex(bklDeep, train_percent, val_percent, test_percent)\n",
        "indexNv = createIndex(nvDeep, train_percent, val_percent, test_percent)\n",
        "print('Indexes Created')\n",
        "\n",
        "# Make sure its randomized to avoid any bias\n",
        "# Save divided data into directories\n",
        "dataPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/'\n",
        "saveImagesToFolders(melDeep, indexMel, 'mel/', dataPath)\n",
        "print('Saved MEL')\n",
        "saveImagesToFolders(bklDeep, indexBkl, 'bkl/', dataPath)\n",
        "print('Saved BKL')\n",
        "saveImagesToFolders(nvDeep, indexMel, 'nv/', dataPath)\n",
        "print('Saved NV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aISDw7xn1G-t"
      },
      "source": [
        "# Train Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGyHLDTI_-E5"
      },
      "source": [
        "# Select Architecture\n",
        "# Choices = 'ResNet50', 'VGG16', 'InceptionResNetV2'\n",
        "# For transfer learning have load_weights = 'imagenet', for non transfer learning load_weights = None\n",
        "model_choice = 'ResNet50'\n",
        "load_weights = 'imagenet'\n",
        "model = get_model(model_choice, load_weights)\n",
        "model.summary()\n",
        "\n",
        "# Set argumemts for the training and validation generator\n",
        "# Probably add vertical and horizontal flip\n",
        "train_args = dict(\n",
        "              rotation_range=0.2,\n",
        "              width_shift_range=0.05,\n",
        "              height_shift_range=0.05,\n",
        "              shear_range=0.05,\n",
        "              zoom_range=0.05,\n",
        "              fill_mode='nearest'\n",
        ")\n",
        "val_args = dict()\n",
        "\n",
        "# Set paths for training and validation data\n",
        "trainPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/train'\n",
        "valPath = '/content/gdrive/Shared drives/CAPSTONE/DeepLearning/Data/deep/val'\n",
        "\n",
        "# Create Data Generators w/ data augmentations (flip, shift, translations, shear, rescale etc)\n",
        "train_gen = ImageDataGenerator(train_args)\n",
        "val_gen = ImageDataGenerator(val_args)\n",
        "\n",
        "# Set Hyperparameters (epochs, learning rate, batchsize, steps, validation steps, cost function)\n",
        "epochs = 100\n",
        "batchsize = 16\n",
        "steps = mth.ceil(1061/batchsize)\n",
        "val_steps = mth.ceil(266/batchsize) \n",
        "learningrate = 0.000001\n",
        "model_name = 'e' + str(epochs) + '_BS' + str(batchsize) + '_lr' + str(learningrate) + '/'\n",
        "ModelPath = '/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/Models/TransferLearning/' + model_choice + '/' + model_name\n",
        "\n",
        "# Specify where the input images are coming from\n",
        "train_gen = train_gen.flow_from_directory(trainPath, target_size=(224,224), batch_size=batchsize)\n",
        "val_gen = val_gen.flow_from_directory(valPath, target_size=(224,224), batch_size=batchsize)\n",
        "\n",
        "\n",
        "if os.path.isdir(ModelPath) == 0 :\n",
        "  os.mkdir(ModelPath)\n",
        "  print('Creating Directory')\n",
        "\n",
        "# Set callbacks using tensorflow functions (csvlogger, early stopping, model checkpoint)\n",
        "model_checkpoint = ModelCheckpoint(ModelPath + 'model.hdf5', monitor='val_loss', save_best_only=True)\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
        "csv_logger = CSVLogger(ModelPath + 'model.csv')\n",
        "\n",
        "# Create Directory Label for specific models\n",
        "# Compile Model Using Hyperparamters [model.compile()]\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(lr=learningrate), metrics=['accuracy'])\n",
        "\n",
        "# Train The model [model.fit()] - This trains the model, logs the data by epoch and saves the best version\n",
        "\n",
        "model_history = model.fit_generator(train_gen, \n",
        "                                    steps_per_epoch = steps, \n",
        "                                    validation_data = val_gen, \n",
        "                                    validation_steps = val_steps, \n",
        "                                    epochs=epochs,\n",
        "                                      callbacks = [model_checkpoint, early_stop, csv_logger])\n",
        "\n",
        "# Once finished training...\n",
        "# Display loss curves to double check it looks good"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqZWkJ-Q1WJg"
      },
      "source": [
        "# Testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz2z1qZMLpgg"
      },
      "source": [
        "# TestDataPaths\n",
        "bklPath = '/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/Data/deep/test/bkl/'\n",
        "melPath = '/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/Data/deep/test/mel/'\n",
        "nvPath = '/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/Data/deep/test/nv/'\n",
        "\n",
        "# Load Model\n",
        "modelPath = '/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/newModels/TransferLearning/ResNet50/e100_BS16_lr1e-05/newmodel.hdf5'\n",
        "model_choice = 'ResNet50'\n",
        "shape = (224, 224, 3) \n",
        "model = get_model(model_choice=model_choice)\n",
        "model.load_weights(modelPath)\n",
        "\n",
        "# Make Predictions\n",
        "bklPredict = getPredictions(model, shape, bklPath)\n",
        "melPredict = getPredictions(model, shape, melPath)\n",
        "nvPredict = getPredictions(model, shape, nvPath)\n",
        "\n",
        "# Create Actuals\n",
        "bklActual = ['bkl'] * len(bklPredict)\n",
        "melActual = ['mel'] * len(melPredict)\n",
        "nvActual = ['nv'] * len(nvPredict)\n",
        "\n",
        "# Concatenate Predictions and Actuals\n",
        "Actual = bklActual + melActual + nvActual\n",
        "Predicted = bklPredict + melPredict + nvPredict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHVuOTHY1ORs"
      },
      "source": [
        "# Evaluate Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrrkYjv9HJVu"
      },
      "source": [
        "# Choose Model\n",
        "dataPath = ModelPath + '/model.csv' #'/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/Models/TransferLearning/ResNet50/e150_BS16_lr5e-07/model.csv'\n",
        "data = pd.read_csv(dataPath)\n",
        "# Plot Loss\n",
        "plotLoss(data)\n",
        "plotAccuracy(data)\n",
        "\n",
        "# Display max/min\n",
        "print(max(data['accuracy']))\n",
        "print(max(data['val_accuracy']))\n",
        "print(min(data['loss']))\n",
        "print(min(data['val_loss']))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fao6bOfdUWMC"
      },
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzTOewp3UUm7"
      },
      "source": [
        "# Confusion Matrices\n",
        "# using panda for confusion matrix\n",
        "act_y = pd.Series(Actual, name = 'Actual')\n",
        "pred_y = pd.Series(Predicted, name = 'Predicted')\n",
        "df_conf = pd.crosstab(act_y, pred_y)\n",
        "print(df_conf)\n",
        "print()\n",
        "\n",
        "#to find some of rows ( not sure if it is require for us)\n",
        "#adding true = margin\n",
        "df_conf1 = pd.crosstab(act_y, pred_y, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "print(df_conf1)\n",
        "print()\n",
        "\n",
        "# if you need to normalize it\n",
        "df_conf_norm = df_conf / df_conf.sum(axis=1)\n",
        "print(df_conf_norm)\n",
        "print()\n",
        "\n",
        "# now for plotting\n",
        "def plot_confusion_matrix(df_conf, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    plt.matshow(df_conf, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_conf.columns))\n",
        "    plt.xticks(tick_marks, df_conf.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_conf.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_conf.index.name)\n",
        "    plt.xlabel(df_conf.columns.name)\n",
        "\n",
        "plot_confusion_matrix(df_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YcgW8vV4Qfv"
      },
      "source": [
        "printConfMatrix()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}