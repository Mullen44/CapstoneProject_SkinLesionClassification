{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisiticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mullen44/CapstoneProject_SkinLesionClassification/blob/main/LogisiticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3X73EtVk_VM"
      },
      "source": [
        "# Import Modules and Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh-NDNonk-H1"
      },
      "source": [
        "#CAPSTONE\n",
        "# Mounting Google Drive\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_UAijzhlDtD"
      },
      "source": [
        "# Import Necessary Libraries and Personal scripts\n",
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/Scripts/\n",
        "from Utils import *\n",
        "from models import *\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn import svm\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymqomqF5lPZh"
      },
      "source": [
        "# Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHponkSYnKMp"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "# Load Feature Extraction models\n",
        "# Model A --> 1000 Class feature extractor\n",
        "modelA = tf.keras.applications.ResNet50()\n",
        "\n",
        "# Model B --> Transfer Learning Model\n",
        "transferPath = 'newModels/TransferLearning/ResNet50/e100_BS16_lr1e-07/newmodel.hdf5'\n",
        "modelB = get_model('ResNet50')\n",
        "modelB.load_weights(transferPath)\n",
        "\n",
        "# Model C --> Normal Learning Model\n",
        "normPath = 'newModels/nonTransferLearning/ResNet50/e100_BS16_lr1e-05/newmodel.hdf5'\n",
        "modelC = get_model('ResNet50')\n",
        "modelC.load_weights(normPath)\n",
        "\n",
        "# Load In Data\n",
        "bklImagePath = 'Data/log/Images/bkl/'\n",
        "melImagePath = 'Data/log/Images/mel/'\n",
        "nvImagePath = 'Data/log/Images/nv/'\n",
        "\n",
        "# Declare dictionaries for each type of image\n",
        "bklImage = dict()\n",
        "melImage = dict()\n",
        "nvImage = dict()\n",
        "\n",
        "# Load in images\n",
        "for filename in os.listdir(bklImagePath) :\n",
        "  bklImage[filename] = imread(bklImagePath + filename)\n",
        "\n",
        "for filename in os.listdir(melImagePath) :\n",
        "  melImage[filename] = imread(melImagePath + filename)\n",
        "\n",
        "for filename in os.listdir(nvImagePath) :\n",
        "  nvImage[filename] = imread(nvImagePath + filename)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzc0JjoSlXm8"
      },
      "source": [
        "# Declare Feature Paths\n",
        "bklFeaturePath = 'Data/log/Features/bkl/'\n",
        "melFeaturePath = 'Data/log/Features/mel/'\n",
        "nvFeaturePath = 'Data/log/Features/nv/'\n",
        "\n",
        "# Make BKL class Predictions\n",
        "for key, value in bklImage.items() :\n",
        "\n",
        "  # Preprocessing of the images\n",
        "  img = np.array(value)\n",
        "  img = cv2.resize(img, (224,224), interpolation=cv2.INTER_LINEAR)\n",
        "  img = img.reshape(224,224,3)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # Feature extraction\n",
        "  predA = np.array(modelA.predict(img)) # 1000 class feature extraction\n",
        "  predB = np.array(modelB.predict(img)) # Transfer Learning 3 Class Prediction Feature extraction\n",
        "  predC = np.array(modelC.predict(img)) # Normal Learning 3 class prediction Feature extraction\n",
        "  label = np.array([0]) # Declare a numerical value that corresponds to the class\n",
        "\n",
        "  # Concatenate 1000+3+3+1=1007 features into one array\n",
        "  featureArray = np.concatenate([predA[0], predB[0], predC[0], label])\n",
        "  # Create the bkl feature name\n",
        "  featureName = bklFeaturePath + key[:-4] + '.npy'\n",
        "\n",
        "  # Save the feature vectors to google drive\n",
        "  np.save(featureName, featureArray)\n",
        "\n",
        "print('Done BKL')\n",
        "\n",
        "for key, value in melImage.items() :\n",
        "  # Preprocessing of the images\n",
        "  img = np.array(value)\n",
        "  img = cv2.resize(img, (224,224), interpolation=cv2.INTER_LINEAR)\n",
        "  img = img.reshape(224,224,3)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # Feature Extraction\n",
        "  predA = np.array(modelA.predict(img)) # 1000 class feature extraction\n",
        "  predB = np.array(modelB.predict(img)) # Transfer learning 3 class prediction feature extraction\n",
        "  predC = np.array(modelC.predict(img)) # Normal learning 3 class prediction feature extraction\n",
        "  label = np.array([1]) # Declare a numerical value that corresponds to the class\n",
        "\n",
        "  # Concatenate 1000+3+3+1=1007 features into one array\n",
        "  featureArray = np.concatenate([predA[0], predB[0], predC[0], label])\n",
        "  # Create the mel feature name\n",
        "  featureName = melFeaturePath + key[:-4] + '.npy'\n",
        "\n",
        "  # Save the feature vectors to google drive\n",
        "  np.save(featureName, featureArray)\n",
        "\n",
        "print('Done Mel')\n",
        "\n",
        "for key, value in nvImage.items() :\n",
        "  # Pre processing of images\n",
        "  img = np.array(value)\n",
        "  img = cv2.resize(img, (224,224), interpolation=cv2.INTER_LINEAR)\n",
        "  img = img.reshape(224,224,3)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "\n",
        "  # Feature Extraction\n",
        "  predA = np.array(modelA.predict(img)) # 1000 Class feature extraction\n",
        "  predB = np.array(modelB.predict(img)) # Transfer learning 3 class prediction feature extraction\n",
        "  predC = np.array(modelC.predict(img)) # Normal learning 3 class prediction feature extraction\n",
        "  label = np.array([2]) # Declare a numerical value that corresponds to the class\n",
        "\n",
        "  # Concatenate 1000+3+3+1=1007 features into one array\n",
        "  featureArray = np.concatenate([predA[0], predB[0], predC[0], label])\n",
        "  # Create the nv feature name\n",
        "  featureName = nvFeaturePath + key[:-4] + '.npy'\n",
        "\n",
        "  # Save feature vectors to google drive\n",
        "  np.save(featureName, featureArray)\n",
        "\n",
        "print('Done NV')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMCUg6Rx_-41"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "bklImagePath = 'Data/log/Images/bkl'\n",
        "melImagePath = 'Data/log/Images/mel'\n",
        "nvImagePath = 'Data/log/Images/nv'\n",
        "bklFeaturePath = 'Data/log/Features/bkl'\n",
        "melFeaturePath = 'Data/log/Features/mel'\n",
        "nvFeaturePath = 'Data/log/Features/nv'\n",
        "\n",
        "print(len(os.listdir(melImagePath)))\n",
        "print(len(os.listdir(melFeaturePath)))\n",
        "print(len(os.listdir(bklImagePath)))\n",
        "print(len(os.listdir(bklFeaturePath)))\n",
        "print(len(os.listdir(nvImagePath)))\n",
        "print(len(os.listdir(nvFeaturePath)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_bmIG-TlWSq"
      },
      "source": [
        "# Data Division"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCJxryOPzUcy"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "# Load In all features from the feature folder\n",
        "bklFeaturePath = 'Data/log/Features/bkl/'\n",
        "melFeaturePath = 'Data/log/Features/mel/'\n",
        "nvFeaturePath = 'Data/log/Features/nv/'\n",
        "\n",
        "# Declare empty vectors with size rows = # of instances and columns = # of features\n",
        "bklSize = (549, 1007)\n",
        "melSize = (556, 1007)\n",
        "nvSize = (559, 1007)\n",
        "bklFeature = np.zeros(bklSize)\n",
        "melFeature = np.zeros(melSize)\n",
        "nvFeature = np.zeros(nvSize)\n",
        "\n",
        "# Load in bkl features\n",
        "i = 0\n",
        "for filename in os.listdir(bklFeaturePath) :\n",
        "  bklFeature[i,:] = np.load(bklFeaturePath + filename)\n",
        "  i += 1\n",
        "\n",
        "# Load in nv features (only one out of every 6 to reduce the number of instances \n",
        "# to a similar size as other classes)\n",
        "i = 0\n",
        "j = 0\n",
        "for filename in os.listdir(nvFeaturePath) :\n",
        "  if i <= 3352//6 :\n",
        "    nvFeature[j,:] = np.load(nvFeaturePath + filename)\n",
        "    j += 1\n",
        "\n",
        "  i += 1\n",
        "\n",
        "# Load in mel features\n",
        "i = 0\n",
        "for filename in os.listdir(melFeaturePath) :\n",
        "  melFeature[i,:] = np.load(melFeaturePath + filename)\n",
        "  i += 1\n",
        "\n",
        "# Divide the data into 80% for training and 20% testing\n",
        "# Randomly get indices for the 80% training\n",
        "bklIndex = indexCreation(bklFeature, 0.8)\n",
        "melIndex = indexCreation(melFeature, 0.8)\n",
        "nvIndex = indexCreation(nvFeature, 0.8)\n",
        "\n",
        "# Declare an empty array for bkl test and training data\n",
        "bklTestFeature = np.zeros((110, 1007))\n",
        "bklTrainFeature = np.zeros((439, 1007))\n",
        "\n",
        "# Declare counters\n",
        "trainCount = 0\n",
        "testCount = 0\n",
        "\n",
        "# Divide the data\n",
        "for i in range(len(bklFeature)) :\n",
        "  if i in bklIndex :\n",
        "    # Place in the bkl train variable\n",
        "    bklTrainFeature[trainCount, :] = bklFeature[i,:]\n",
        "    trainCount += 1\n",
        "\n",
        "  else :\n",
        "    # Place in bkl test variable\n",
        "    bklTestFeature[testCount, :] = bklFeature[i,:]\n",
        "    testCount += 1\n",
        "\n",
        "# Declare an empty array for mel test and training\n",
        "melTrainFeature = np.zeros((int(0.8*556), 1007))\n",
        "melTestFeature = np.zeros((556-int(0.8*556), 1007))\n",
        "\n",
        "# Declare counters\n",
        "trainCount = 0\n",
        "testCount = 0\n",
        "\n",
        "# Divide the data\n",
        "for i in range(len(melFeature)) :\n",
        "  if i in melIndex :\n",
        "    # Place in the mel train variable\n",
        "    melTrainFeature[trainCount, :] = melFeature[i,:]\n",
        "    trainCount += 1\n",
        "\n",
        "  else :\n",
        "    # Place in mel test variable\n",
        "    melTestFeature[testCount, :] = melFeature[i,:]\n",
        "    testCount += 1\n",
        "\n",
        "# Declare empty arrays for mel train and test\n",
        "nvTrainFeature = np.zeros((int(0.8*559), 1007))\n",
        "nvTestFeature = np.zeros((559-int(0.8*559), 1007))\n",
        "\n",
        "# Declare counters\n",
        "trainCount = 0\n",
        "testCount = 0\n",
        "\n",
        "# Divide the data\n",
        "for i in range(len(nvFeature)) :\n",
        "  if i in nvIndex :\n",
        "    # Place in nv train variable\n",
        "    nvTrainFeature[trainCount, :] = nvFeature[i,:]\n",
        "    trainCount += 1\n",
        "\n",
        "  else :\n",
        "    # Place in nv test variable\n",
        "    nvTestFeature[testCount, :] = nvFeature[i,:]\n",
        "    testCount += 1\n",
        "\n",
        "# Stack the 3 classes on top of eachother\n",
        "train = np.vstack([melTrainFeature, bklTrainFeature, nvTrainFeature])\n",
        "test = np.vstack([melTestFeature, bklTestFeature, nvTestFeature])\n",
        "\n",
        "# Save to google drive\n",
        "np.save('Data/log/Features/test.npy', test)\n",
        "np.save('Data/log/Features/train.npy', train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usc-rVbJlcEQ"
      },
      "source": [
        "# Logistic Regression Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avsVvjbU26uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8795283-4455-44da-e45b-468295cca84d"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "# Load in training and testing data\n",
        "dataTrain = np.load('Data/log/Features/train.npy')\n",
        "dataTest = np.load('Data/log/Features/test.npy')\n",
        "\n",
        "# Divide into X and y for training and testing\n",
        "# X is the features and y is the labels at the end\n",
        "X_train = dataTrain[:, 0:1006]\n",
        "y_train = dataTrain[:, 1006]\n",
        "X_test = dataTest[:, 0:1006]\n",
        "y_test = dataTest[:, 1006]\n",
        "\n",
        "# Declare parameters\n",
        "num_Folds = 7\n",
        "num_iterations = 10000\n",
        "\n",
        "# Declare model\n",
        "model = LogisticRegressionCV(cv = num_Folds, verbose = 2, max_iter = num_iterations)\n",
        "\n",
        "#model = svm.SVC(decision_function_shape='ovo')\n",
        "# Fit/train model\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Declare model path\n",
        "modelPath = 'Models/LogRegModel/' + str(num_Folds) + '_Fold_' + str(num_iterations) + '_iter.pkl'\n",
        "# Save model\n",
        "pickle.dump(model, open(modelPath, 'wb'))\n",
        "\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shared drives/CAPSTONE/DeepLearning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHJnfHI6liIh"
      },
      "source": [
        "# Testing/Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpQb2Rup3FEi"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "# Load in testing data\n",
        "dataTest = np.load('Data/log/Features/test.npy')\n",
        "X_test = dataTest[:, 0:1006]\n",
        "y_test = dataTest[:, 1006]\n",
        "\n",
        "# Load in model\n",
        "model = pickle.load(open('Models/LogRegModel/7_Fold_10000_iter.pkl', 'rb'))\n",
        "\n",
        "# Test Model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print('Score:', model.score(X_test, y_test))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "print(metrics.classification_report(y_test, y_pred, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgWurQL_RKV3"
      },
      "source": [
        "## Create TF Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7pM77Z8G7Or"
      },
      "source": [
        "def indexMax(input) :\n",
        "  return np.argmax(np.array(input))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1eRKUkNQLDw"
      },
      "source": [
        "#############\n",
        "\n",
        "# Load sklearn logistic regression model\n",
        "model = pickle.load(open('Models/LogRegModel/7_Fold_10000_iter.pkl', 'rb'))\n",
        "\n",
        "# create a TF model with the same architecture\n",
        "tf_model = tf.keras.models.Sequential()\n",
        "tf_model.add(tf.keras.Input(shape=(1006)))\n",
        "tf_model.add(tf.keras.layers.Dense(3))\n",
        "\n",
        "\n",
        "# assign the parameters from sklearn to the TF model\n",
        "tf_model.layers[0].weights[0].assign(model.coef_.transpose())\n",
        "tf_model.layers[0].bias.assign(model.intercept_.transpose())\n",
        "\n",
        "# Save the new tensorflow version of the model\n",
        "tf_model.save('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/TF/logRegModel.hdf5')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for i in range(len(X_test)) :\n",
        "  temp = np.expand_dims(X_test[i], axis=0)\n",
        "  tflow = indexMax(tf_model(temp))\n",
        "  sk = model.predict(temp)\n",
        "  print('Input # %d' %i)\n",
        "  print(tf_model(temp))\n",
        "  print(tflow)\n",
        "  print(sk)\n",
        "  if tflow != sk :\n",
        "    print('FLAG')\n",
        "    count+=1\n",
        "\n",
        "print ('Logistic TensorFlow Model is type:', type(tf_model))\n",
        "\n",
        "print(count)\n",
        "\n",
        "# verify the models do the same prediction\n",
        "#assert np.all((tf_model(x) > 0)[:, 0].numpy() == model.predict(x))\n",
        "################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgcxo0NQdU7z"
      },
      "source": [
        "tf_model.save('/content/gdrive/Shareddrives/CAPSTONE/DeepLearning/TF/logRegModel.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BczQDncj7jtb"
      },
      "source": [
        "act_y = pd.Series(y_test, name = 'Actual')\n",
        "pred_y = pd.Series(y_pred, name = 'Predicted')\n",
        "df_conf = pd.crosstab(act_y, pred_y)\n",
        "print(df_conf)\n",
        "\n",
        "df_conf1 = pd.crosstab(act_y, pred_y, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "print(df_conf1)\n",
        "\n",
        "# if you need to normalize it\n",
        "df_conf_norm = df_conf / df_conf.sum(axis=1)\n",
        "print(df_conf_norm1)\n",
        "print()\n",
        "\n",
        "# now for plotting\n",
        "def plot_confusion_matrix(df_conf, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    plt.matshow(df_conf, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(df_conf.columns))\n",
        "    plt.xticks(tick_marks, df_conf.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_conf.index)\n",
        "    \n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_conf.index.name)\n",
        "    plt.xlabel(df_conf.columns.name)\n",
        "\n",
        "plot_confusion_matrix(df_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia6jXArG6ZI8"
      },
      "source": [
        "def changeLabels(data) :\n",
        "  out = []\n",
        "  for i in range(len(data)) :\n",
        "    if data[i] == 0.0 :\n",
        "      out.append('bkl')\n",
        "    elif data[i] == 1.0 :\n",
        "      out.append('mel')\n",
        "    else :\n",
        "      out.append('nv')\n",
        "x = pd.Series(changeLabels(y_test))\n",
        "y = pd.Series(changeLabels(y_pred))\n",
        "\n",
        "df_conf = pd.crosstab(x, y)\n",
        "print(df_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_osQHboIw-W4"
      },
      "source": [
        "# Package Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXdoJRh09HMc"
      },
      "source": [
        "%cd /content/gdrive/Shared drives/CAPSTONE/DeepLearning/\n",
        "# Load Deep Learning Models\n",
        "# Model A --> 1000 Class feature extractor\n",
        "modelA = tf.keras.applications.ResNet50()\n",
        "\n",
        "# Model B --> Transfer Learning Model\n",
        "transferPath = 'newModels/TransferLearning/ResNet50/e100_BS16_lr1e-07/newmodel.hdf5'\n",
        "modelB = get_model('ResNet50')\n",
        "modelB.load_weights(transferPath)\n",
        "\n",
        "# Model C --> Normal Learning Model\n",
        "normPath = 'newModels/nonTransferLearning/ResNet50/e100_BS16_lr1e-05/newmodel.hdf5'\n",
        "modelC = get_model('ResNet50')\n",
        "modelC.load_weights(normPath)\n",
        "\n",
        "# Model D Load Logistic Regression Model\n",
        "logPath = 'Models/LogRegModel/7_Fold_10000_iter.pkl'\n",
        "model = pickle.load(open(logPath, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBurVpHoZZ21"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import layers\n",
        "import itertools\n",
        "\n",
        "\n",
        "def define_stacked_model2(modelA, modelB, modelC):\n",
        "  \n",
        "  # Update all laters in all models to not be trainable\n",
        "  \n",
        "  for layer in modelA.layers:\n",
        "    # Make not trainable\n",
        "    layers.trainable = False\n",
        "    # Rename to avoid unique layer name issue\n",
        "    layer._name ='ensemble_A_' +layer.name\n",
        "\n",
        "  for layer in modelB.layers:\n",
        "    # Make not trainable\n",
        "    layers.trainable = False\n",
        "    # Rename to avoid unique layer name issue\n",
        "    layer._name ='ensemble_B_' +layer.name\n",
        "\n",
        "  for layer in modelC.layers:\n",
        "    # Make not trainable\n",
        "    layers.trainable = False\n",
        "    # Rename to avoid unique layer name issue\n",
        "    layer._name ='ensemble_C_' +layer.name\n",
        "\n",
        "  # Define multi-headed input\n",
        "  ensemble_visible = [modelA.input, modelB.input, modelC.input]\n",
        "  # Define output\n",
        "  ensemble_output = [modelA.output, modelB.output, modelC.output]\n",
        "\n",
        "  # Create model\n",
        "  model = Model(inputs=ensemble_visible, outputs=ensemble_output)\n",
        "\n",
        "  # Compile\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSDq8UgpieSJ"
      },
      "source": [
        "ensembleModel = define_stacked_model2(modelA, modelB, modelC)\n",
        "#ensembleModel.summary()\n",
        "ensembleModel.save('ensembleModels/deepModel.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmtq8v1d_MtJ"
      },
      "source": [
        "ensembleModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
